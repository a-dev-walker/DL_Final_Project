{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## pip if needed\n!pip install neptune\n#!pip install -U 'neptune-client'\n!pip install torchsummary","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport copy\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nimport pytorch_lightning as pl\nimport torch.nn.functional as F\nimport torch.optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom sklearn.model_selection import train_test_split\nfrom torchsummary import summary\nimport neptune\n#from neptune.new.integrations.pytorch_lightning import NeptuneLogger\n\n\npl.seed_everything(2023)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Setting up neptune logging\n\nrun = neptune.init_run(\n    project=\"a-dev-walker/DL-final-project\",\n    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIyMmVjNDlmYS04MjZmLTQ1N2QtODUxMi1lNTdmZGQzMzNhMzUifQ==\",\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Downloading the Data and labels into the working space\nlabels_df = pd.read_csv('/kaggle/input/histopathologic-cancer-detection/train_labels.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Investigating the data a little bit to see the label counts\nprint(labels_df['label'].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Creating the dataset object\n\nclass HistopathDataset(Dataset):\n    def __init__(self, data_loc, image_ids, labels, transform=None):\n        self.data_loc = data_loc\n        self.image_ids = image_ids\n        self.labels = labels\n        self.transform = transform\n        \n        self.image_file_names = [image_id + \".tif\" for image_id in self.image_ids]\n        \n        self.image_file_paths = [os.path.join(self.data_loc, file) for file in self.image_file_names ]\n        \n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        img = Image.open(self.image_file_paths[idx])\n        label = self.labels.iloc[idx]\n\n        if self.transform:\n            img = self.transform(img)\n\n        return img, label\n\n    \n    \n#For the train transform we want to augment the images somewhat to teach the system a variety of orientations\n#This is done even though histopathology iamges are not orientated but there could be some bias in the capture that we'd like to eliminate\ntrain_transform = transforms.Compose([\n    transforms.Resize((96, 96)),\n    transforms.RandomHorizontalFlip(p=0.5), \n    transforms.RandomVerticalFlip(p=0.5),  \n    transforms.RandomRotation(45), \n    transforms.ToTensor()\n])  \n\n    \n#Test and Validation transform just needs to turn to tensors\ntest_val_transform = transforms.Compose([\n    transforms.Resize((96, 96)),\n    transforms.ToTensor()\n])\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Splitting the data, putting it into a dataset, and using a dataloader\n\ndata_location = \"/kaggle/input/histopathologic-cancer-detection/train\"\nBATCH_SIZE = 32\n\noverall_labels = labels_df['label']\noverall_labels = overall_labels.reset_index(drop=True)\n\n\n# Doing test train split\nX_train, X_val_test, y_train, y_val_test = train_test_split(labels_df['id'], overall_labels, test_size=.3)\nX_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size = .5)\n\n# Creating datasets\ntrain_dataset = HistopathDataset(data_loc = data_location, image_ids = X_train,labels = y_train, transform = train_transform)\nval_dataset = HistopathDataset(data_loc = data_location, image_ids = X_val,labels = y_val, transform = test_val_transform)\ntest_dataset = HistopathDataset(data_loc = data_location, image_ids = X_test,labels = y_test, transform = test_val_transform)\n\n# Creating data loaders\ntrain_dataloader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True)\nval_dataloader = DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle = True)\ntest_dataloader = DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle = True)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the lenghts of all of the datasets for QC\n\nprint(\"train dataset size:\", len(train_dataset))\nprint(\"validation dataset size:\", len(val_dataset))\nprint(\"test dataset size:\", len(test_dataset))\n\n\n\nprint(\"train dataloader size:\", len(train_dataloader))\nprint(\"validation dataloader size:\", len(val_dataloader))\nprint(\"test dataloader size:\", len(test_dataloader))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Creating the Model architecture\n\nclass HistopathClassifier(pl.LightningModule):\n    def __init__(self):\n        super(HistopathClassifier, self).__init__()\n        \n        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(64 * 12 * 12, 512)\n        self.fc2 = nn.Linear(512, 1)\n        self.dropout = nn.Dropout(0.25)\n        self.sigmoid = nn.Sigmoid()\n        self.loss = nn.BCELoss()\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.pool(F.relu(self.conv3(x)))\n        x = x.view(-1, 64 * 12 * 12)\n        x = self.dropout(x)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return self.sigmoid(x)\n    \n    def train_step(self, inputs, labels, criterion, optimizer):\n        optimizer.zero_grad()\n        outputs = self(inputs).squeeze()\n        \n        if outputs.squeeze().dim() == 0: #done for rare instances where a batch could have only 1 sample\n            outputs = outputs.unsqueeze(0)\n    \n        #print(outputs.shape)\n        #print(outputs.squeeze().shape)\n        #print(labels.float().shape)\n        loss = criterion(outputs, labels.float())\n        loss.backward()\n        optimizer.step()\n        return loss.item()\n\n    def validate_step(self, inputs, labels, criterion):\n        with torch.no_grad():\n            outputs = self(inputs).squeeze()\n            if outputs.squeeze().dim() == 0: #done for rare instances where a batch could have only 1 sample\n                outputs = outputs.unsqueeze(0)\n            \n            loss = criterion(outputs, labels.float())\n        return loss.item()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Creating the model and getting a summary\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nnum_epochs = 20\nmodel = HistopathClassifier()\nmodel.to(device)\n\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n\n\nsummary(model, input_size=(3, 96, 96),device=device.type)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Going about training the model utilzing neptune to log values\n\nweight_path = \"weights.pt\"\nbest_loss=float('inf') # initialize best loss to a large value\n\nrun[\"config/model\"] = type(model).__name__\nrun[\"config/criterion\"] = type(criterion).__name__\nrun[\"config/optimizer\"] = type(optimizer).__name__\n\n\nfor epoch in range(num_epochs):\n    running_loss = 0.0\n    for i, data in enumerate(train_dataloader, 0):\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        loss = model.train_step(inputs, labels, criterion, optimizer)\n        # if (i%100 == 0): print(loss)\n        running_loss += loss\n\n    # Log training loss to Neptune\n    epoch_loss = running_loss / (i + 1)\n    run[\"train/loss\"].log(epoch_loss)\n    \n    # Validation\n    val_loss = 0.0\n    for i, data in enumerate(val_dataloader, 0):\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        loss = model.validate_step(inputs, labels, criterion)\n        val_loss += loss\n        \n    # Getting the epoch validation loss    \n    epoch_val_loss = val_loss / (i + 1)\n    \n    # Log validation loss to Neptune and update the scheduler\n    run[\"val/loss\"].log(epoch_val_loss)\n    scheduler.step(epoch_val_loss)\n\n    print(f\"Epoch: {epoch + 1}, Train Loss: {epoch_loss}, Val Loss: {epoch_val_loss}\")\n\n        \n    # Saving the best model to neptune for use down the line\n    if(epoch_val_loss < best_loss):\n        best_loss = epoch_val_loss\n        best_model_wts = copy.deepcopy(model.state_dict())\n            \n        # Store weights into a local file that will be uploaded\n        torch.save(model.state_dict(), weight_path)\n        run[\"model_checkpoints/my_model\"].upload(\"/kaggle/working/weights.pt\")\n        print(\"Copied best model weights!\")\n    \n    \n    \nprint(\"Finished Training\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Getting test loss on the trained model as well as the test accuarcy\n\ntest_loss = 0.0\ncorrect = 0\ntotal = 0\n\n#model.load_state_dict(torch.load(\"/kaggle/working/weights.pt\"))\nmodel.load_state_dict(torch.load(\"/kaggle/working/weights.pt\"))\n\n\nwith torch.no_grad():\n    for data in test_dataloader:\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        \n        loss = criterion(outputs.squeeze(), labels.float())\n        test_loss += loss.item()\n\n        # Calculate accuracy\n        predicted = torch.round(outputs.squeeze())\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n# Log test loss and accuracy to Neptune\ntest_loss = test_loss / len(test_dataloader)\ntest_accuracy = correct / total\n#run[\"test/loss\"].log(test_loss)\n#run[\"test/accuracy\"].log(test_accuracy)\n\nprint(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n\n# Close the Neptune run after logging all metrics\nrun.stop()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Creating the ROC curve for the model\nfrom sklearn.metrics import roc_curve, auc\n\nmodel.eval()\n\n# Initialize lists to store true labels and predicted probabilities\ntrue_labels = []\npredicted_probs = []\n\nwith torch.no_grad():\n    for inputs, labels in test_dataloader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        # Store true labels\n        true_labels.extend(labels.cpu().numpy())\n        \n        # Get predicted probabilities and store them\n        outputs = model(inputs)\n        predicted_probs.extend(outputs.cpu().numpy().squeeze())\n\n# Convert lists to numpy arrays\ntrue_labels = np.array(true_labels)\npredicted_probs = np.array(predicted_probs)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Plotting the ROC curve for the model\n\nimport matplotlib.pyplot as plt\n\nfpr, tpr, _ = roc_curve(true_labels, predicted_probs)\nroc_auc = auc(fpr, tpr)\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC)')\nplt.legend(loc=\"lower right\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}