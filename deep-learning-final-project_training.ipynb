{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## pip if needed\n!pip install neptune\n#!pip install -U 'neptune-client'\n!pip install torchsummary","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-04-30T06:16:27.593040Z","iopub.execute_input":"2023-04-30T06:16:27.593490Z","iopub.status.idle":"2023-04-30T06:16:57.375803Z","shell.execute_reply.started":"2023-04-30T06:16:27.593449Z","shell.execute_reply":"2023-04-30T06:16:57.374468Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting neptune\n  Using cached neptune-1.1.1-py3-none-any.whl (442 kB)\nRequirement already satisfied: future>=0.17.1 in /opt/conda/lib/python3.7/site-packages (from neptune) (0.18.3)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from neptune) (1.3.5)\nRequirement already satisfied: Pillow>=1.1.6 in /opt/conda/lib/python3.7/site-packages (from neptune) (9.4.0)\nRequirement already satisfied: urllib3 in /opt/conda/lib/python3.7/site-packages (from neptune) (1.26.14)\nRequirement already satisfied: oauthlib>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from neptune) (3.2.2)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from neptune) (1.16.0)\nCollecting swagger-spec-validator>=2.7.4\n  Using cached swagger_spec_validator-3.0.3-py2.py3-none-any.whl (27 kB)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from neptune) (4.11.4)\nRequirement already satisfied: requests>=2.20.0 in /opt/conda/lib/python3.7/site-packages (from neptune) (2.28.2)\nRequirement already satisfied: GitPython>=2.0.8 in /opt/conda/lib/python3.7/site-packages (from neptune) (3.1.30)\nCollecting bravado<12.0.0,>=11.0.0\n  Using cached bravado-11.0.3-py2.py3-none-any.whl (38 kB)\nRequirement already satisfied: boto3>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from neptune) (1.26.100)\nRequirement already satisfied: requests-oauthlib>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from neptune) (1.3.1)\nRequirement already satisfied: PyJWT in /opt/conda/lib/python3.7/site-packages (from neptune) (2.6.0)\nRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.7/site-packages (from neptune) (8.1.3)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from neptune) (23.0)\nRequirement already satisfied: websocket-client!=1.0.0,>=0.35.0 in /opt/conda/lib/python3.7/site-packages (from neptune) (1.4.2)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from neptune) (5.9.3)\nRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.16.0->neptune) (0.6.0)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.16.0->neptune) (1.0.1)\nCollecting botocore<1.30.0,>=1.29.100\n  Using cached botocore-1.29.123-py3-none-any.whl (10.7 MB)\nRequirement already satisfied: simplejson in /opt/conda/lib/python3.7/site-packages (from bravado<12.0.0,>=11.0.0->neptune) (3.18.4)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from bravado<12.0.0,>=11.0.0->neptune) (4.4.0)\nRequirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from bravado<12.0.0,>=11.0.0->neptune) (2.8.2)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from bravado<12.0.0,>=11.0.0->neptune) (6.0)\nCollecting monotonic\n  Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nCollecting bravado-core>=5.16.1\n  Using cached bravado_core-5.17.1-py2.py3-none-any.whl (67 kB)\nRequirement already satisfied: msgpack in /opt/conda/lib/python3.7/site-packages (from bravado<12.0.0,>=11.0.0->neptune) (1.0.4)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from GitPython>=2.0.8->neptune) (4.0.10)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20.0->neptune) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20.0->neptune) (2022.12.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20.0->neptune) (2.1.1)\nRequirement already satisfied: jsonschema in /opt/conda/lib/python3.7/site-packages (from swagger-spec-validator>=2.7.4->neptune) (4.17.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->neptune) (3.11.0)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->neptune) (2023.3)\nRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.7/site-packages (from pandas->neptune) (1.21.6)\nCollecting jsonref\n  Using cached jsonref-1.1.0-py3-none-any.whl (9.4 kB)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune) (5.0.0)\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (0.19.3)\nRequirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (22.2.0)\nRequirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (5.10.2)\nRequirement already satisfied: pkgutil-resolve-name>=1.3.10 in /opt/conda/lib/python3.7/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (1.3.10)\nCollecting jsonpointer>1.13\n  Using cached jsonpointer-2.3-py2.py3-none-any.whl (7.8 kB)\nCollecting fqdn\n  Using cached fqdn-1.5.1-py3-none-any.whl (9.1 kB)\nCollecting uri-template\n  Using cached uri_template-1.2.0-py3-none-any.whl (10 kB)\nCollecting rfc3339-validator\n  Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\nCollecting isoduration\n  Using cached isoduration-20.11.0-py3-none-any.whl (11 kB)\nCollecting rfc3987\n  Using cached rfc3987-1.3.8-py2.py3-none-any.whl (13 kB)\nCollecting webcolors>=1.11\n  Using cached webcolors-1.13-py3-none-any.whl (14 kB)\nRequirement already satisfied: cached-property>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from fqdn->jsonschema->swagger-spec-validator>=2.7.4->neptune) (1.5.2)\nRequirement already satisfied: arrow>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from isoduration->jsonschema->swagger-spec-validator>=2.7.4->neptune) (1.2.3)\nInstalling collected packages: rfc3987, monotonic, webcolors, uri-template, rfc3339-validator, jsonref, jsonpointer, fqdn, botocore, swagger-spec-validator, isoduration, bravado-core, bravado, neptune\n  Attempting uninstall: botocore\n    Found existing installation: botocore 1.27.59\n    Uninstalling botocore-1.27.59:\n      Successfully uninstalled botocore-1.27.59\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\naiobotocore 2.4.2 requires botocore<1.27.60,>=1.27.59, but you have botocore 1.29.123 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed botocore-1.29.123 bravado-11.0.3 bravado-core-5.17.1 fqdn-1.5.1 isoduration-20.11.0 jsonpointer-2.3 jsonref-1.1.0 monotonic-1.6 neptune-1.1.1 rfc3339-validator-0.1.4 rfc3987-1.3.8 swagger-spec-validator-3.0.3 uri-template-1.2.0 webcolors-1.13\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: torchsummary in /opt/conda/lib/python3.7/site-packages (1.5.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport copy\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nimport pytorch_lightning as pl\nimport torch.nn.functional as F\nimport torch.optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom sklearn.model_selection import train_test_split\nfrom torchsummary import summary\nimport neptune\n#from neptune.new.integrations.pytorch_lightning import NeptuneLogger\n\n\npl.seed_everything(2023)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2023-04-30T06:16:57.832052Z","iopub.execute_input":"2023-04-30T06:16:57.832780Z","iopub.status.idle":"2023-04-30T06:16:57.846838Z","shell.execute_reply.started":"2023-04-30T06:16:57.832719Z","shell.execute_reply":"2023-04-30T06:16:57.845082Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"2023"},"metadata":{}}]},{"cell_type":"code","source":"## Setting up neptune logging\n\n# neptune_logger = NeptuneLogger(\n#     project=\"a-dev-walker/DL-final-project\",\n#     api_key=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIyMmVjNDlmYS04MjZmLTQ1N2QtODUxMi1lNTdmZGQzMzNhMzUifQ==\",\n#     log_model_checkpoints=False,\n# )\n\nrun = neptune.init_run(\n    project=\"a-dev-walker/DL-final-project\",\n    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIyMmVjNDlmYS04MjZmLTQ1N2QtODUxMi1lNTdmZGQzMzNhMzUifQ==\",\n)","metadata":{"execution":{"iopub.status.busy":"2023-04-30T06:16:19.053612Z","iopub.execute_input":"2023-04-30T06:16:19.054092Z","iopub.status.idle":"2023-04-30T06:16:19.116458Z","shell.execute_reply.started":"2023-04-30T06:16:19.054052Z","shell.execute_reply":"2023-04-30T06:16:19.112379Z"},"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_24/2262108646.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m run = neptune.init_run(\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"a-dev-walker/DL-final-project\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mapi_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIyMmVjNDlmYS04MjZmLTQ1N2QtODUxMi1lNTdmZGQzMzNhMzUifQ==\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'neptune' is not defined"],"ename":"NameError","evalue":"name 'neptune' is not defined","output_type":"error"}]},{"cell_type":"code","source":"## Downloading the Data and labels into the working space\nlabels_df = pd.read_csv('/kaggle/input/histopathologic-cancer-detection/train_labels.csv')","metadata":{"execution":{"iopub.status.busy":"2023-04-30T06:17:03.671263Z","iopub.execute_input":"2023-04-30T06:17:03.671657Z","iopub.status.idle":"2023-04-30T06:17:04.167270Z","shell.execute_reply.started":"2023-04-30T06:17:03.671621Z","shell.execute_reply":"2023-04-30T06:17:04.166061Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"## Investigating the data a little bit to see the label counts\nprint(labels_df['label'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2023-04-30T06:17:06.218790Z","iopub.execute_input":"2023-04-30T06:17:06.219864Z","iopub.status.idle":"2023-04-30T06:17:06.242938Z","shell.execute_reply.started":"2023-04-30T06:17:06.219825Z","shell.execute_reply":"2023-04-30T06:17:06.240268Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"0    130908\n1     89117\nName: label, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"## Creating the dataset\n\nclass HistopathDataset(Dataset):\n    def __init__(self, data_loc, image_ids, labels, transform=None):\n        self.data_loc = data_loc\n        self.image_ids = image_ids\n        self.labels = labels\n        self.transform = transform\n        \n        self.image_file_names = [image_id + \".tif\" for image_id in self.image_ids]\n        \n        self.image_file_paths = [os.path.join(self.data_loc, file) for file in self.image_file_names ]\n        \n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        img = Image.open(self.image_file_paths[idx])\n        label = self.labels.iloc[idx]\n\n        if self.transform:\n            img = self.transform(img)\n\n        return img, label\n\n    \n    \n#For the train transform we want to augment the images somewhat to teach the system a variety of orientations\n#This is done even though histopathology iamges are not orientated but there could be some bias in the capture that we'd like to eliminate\ntrain_transform = transforms.Compose([\n    transforms.Resize((96, 96)),\n    transforms.RandomHorizontalFlip(p=0.5), \n    transforms.RandomVerticalFlip(p=0.5),  \n    transforms.RandomRotation(45), \n    transforms.ToTensor()\n])  \n\n    \n    \n#Test and Validation transform just needs to turn to tensors\ntest_val_transform = transforms.Compose([\n    transforms.Resize((96, 96)),\n    transforms.ToTensor()\n])\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-30T06:17:08.753575Z","iopub.execute_input":"2023-04-30T06:17:08.754111Z","iopub.status.idle":"2023-04-30T06:17:08.794055Z","shell.execute_reply.started":"2023-04-30T06:17:08.754067Z","shell.execute_reply":"2023-04-30T06:17:08.792532Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"## Splitting the data, putting it into a dataset, and using a dataloader\n\ndata_location = \"/kaggle/input/histopathologic-cancer-detection/train\"\nBATCH_SIZE = 32\n\noverall_labels = labels_df['label']\n\noverall_labels = overall_labels.reset_index(drop=True)\n\n\nX_train, X_val_test, y_train, y_val_test = train_test_split(labels_df['id'], overall_labels, test_size=.3)\nX_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size = .5)\n\ntrain_dataset = HistopathDataset(data_loc = data_location, image_ids = X_train,labels = y_train, transform = train_transform)\nval_dataset = HistopathDataset(data_loc = data_location, image_ids = X_val,labels = y_val, transform = test_val_transform)\ntest_dataset = HistopathDataset(data_loc = data_location, image_ids = X_test,labels = y_test, transform = test_val_transform)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True)\nval_dataloader = DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle = True)\ntest_dataloader = DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle = True)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-30T06:17:14.172786Z","iopub.execute_input":"2023-04-30T06:17:14.173371Z","iopub.status.idle":"2023-04-30T06:17:14.624087Z","shell.execute_reply.started":"2023-04-30T06:17:14.173330Z","shell.execute_reply":"2023-04-30T06:17:14.623051Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"print(\"train dataset size:\", len(train_dataloader))\nprint(\"validation dataset size:\", len(val_dataloader))\n","metadata":{"execution":{"iopub.status.busy":"2023-04-30T06:17:19.010024Z","iopub.execute_input":"2023-04-30T06:17:19.010401Z","iopub.status.idle":"2023-04-30T06:17:19.017155Z","shell.execute_reply.started":"2023-04-30T06:17:19.010368Z","shell.execute_reply":"2023-04-30T06:17:19.015770Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"train dataset size: 4814\nvalidation dataset size: 1032\n","output_type":"stream"}]},{"cell_type":"code","source":"## Creating the model\n\nclass HistopathClassifier(pl.LightningModule):\n    def __init__(self):\n        super(HistopathClassifier, self).__init__()\n        \n        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(64 * 12 * 12, 512)\n        self.fc2 = nn.Linear(512, 1)\n        self.dropout = nn.Dropout(0.25)\n        self.sigmoid = nn.Sigmoid()\n        self.loss = nn.BCELoss()\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.pool(F.relu(self.conv3(x)))\n        x = x.view(-1, 64 * 12 * 12)\n        x = self.dropout(x)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return self.sigmoid(x)\n    \n    def train_step(self, inputs, labels, criterion, optimizer):\n        optimizer.zero_grad()\n        outputs = self(inputs).squeeze()\n        \n        if outputs.squeeze().dim() == 0: #done for rare instances where a batch could have only 1 sample\n            outputs = outputs.unsqueeze(0)\n    \n        #print(outputs.shape)\n        #print(outputs.squeeze().shape)\n        #print(labels.float().shape)\n        loss = criterion(outputs, labels.float())\n        loss.backward()\n        optimizer.step()\n        return loss.item()\n\n    def validate_step(self, inputs, labels, criterion):\n        with torch.no_grad():\n            outputs = self(inputs).squeeze()\n            if outputs.squeeze().dim() == 0: #done for rare instances where a batch could have only 1 sample\n                outputs = outputs.unsqueeze(0)\n            \n            loss = criterion(outputs, labels.float())\n        return loss.item()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-30T06:17:21.019954Z","iopub.execute_input":"2023-04-30T06:17:21.020367Z","iopub.status.idle":"2023-04-30T06:17:21.051914Z","shell.execute_reply.started":"2023-04-30T06:17:21.020327Z","shell.execute_reply":"2023-04-30T06:17:21.050806Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"## Creating the model and getting a summary\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nnum_epochs = 20\nmodel = HistopathClassifier()\nmodel.to(device)\n\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n\n\nsummary(model, input_size=(3, 96, 96),device=device.type)","metadata":{"execution":{"iopub.status.busy":"2023-04-30T06:17:24.821045Z","iopub.execute_input":"2023-04-30T06:17:24.822216Z","iopub.status.idle":"2023-04-30T06:17:38.678386Z","shell.execute_reply.started":"2023-04-30T06:17:24.822172Z","shell.execute_reply":"2023-04-30T06:17:38.677188Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 16, 96, 96]             448\n         MaxPool2d-2           [-1, 16, 48, 48]               0\n            Conv2d-3           [-1, 32, 48, 48]           4,640\n         MaxPool2d-4           [-1, 32, 24, 24]               0\n            Conv2d-5           [-1, 64, 24, 24]          18,496\n         MaxPool2d-6           [-1, 64, 12, 12]               0\n           Dropout-7                 [-1, 9216]               0\n            Linear-8                  [-1, 512]       4,719,104\n           Dropout-9                  [-1, 512]               0\n           Linear-10                    [-1, 1]             513\n          Sigmoid-11                    [-1, 1]               0\n================================================================\nTotal params: 4,743,201\nTrainable params: 4,743,201\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.11\nForward/backward pass size (MB): 2.54\nParams size (MB): 18.09\nEstimated Total Size (MB): 20.74\n----------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"## Going about training the model utilzing neptune to log values\n\nweight_path = \"weights.pt\"\nbest_loss=float('inf') # initialize best loss to a large value\n\nrun[\"config/model\"] = type(model).__name__\nrun[\"config/criterion\"] = type(criterion).__name__\nrun[\"config/optimizer\"] = type(optimizer).__name__\n\n\nfor epoch in range(num_epochs):\n    running_loss = 0.0\n    for i, data in enumerate(train_dataloader, 0):\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        loss = model.train_step(inputs, labels, criterion, optimizer)\n        # if (i%100 == 0): print(loss)\n        running_loss += loss\n\n    # Log training loss to Neptune\n    epoch_loss = running_loss / (i + 1)\n    run[\"train/loss\"].log(epoch_loss)\n    \n    # Validation\n    val_loss = 0.0\n    for i, data in enumerate(val_dataloader, 0):\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        loss = model.validate_step(inputs, labels, criterion)\n        val_loss += loss\n        \n    # Getting the epoch validation loss    \n    epoch_val_loss = val_loss / (i + 1)\n    \n    # Log validation loss to Neptune and update the scheduler\n    run[\"val/loss\"].log(epoch_val_loss)\n    scheduler.step(epoch_val_loss)\n\n    print(f\"Epoch: {epoch + 1}, Train Loss: {epoch_loss}, Val Loss: {epoch_val_loss}\")\n\n        \n    # Saving the best model to neptune for use down the line\n    if(epoch_val_loss < best_loss):\n        best_loss = epoch_val_loss\n        best_model_wts = copy.deepcopy(model.state_dict())\n            \n        # Store weights into a local file that will be uploaded\n        torch.save(model.state_dict(), weight_path)\n        run[\"model_checkpoints/my_model\"].upload(\"/kaggle/working/weights.pt\")\n        print(\"Copied best model weights!\")\n    \n    \n    \nprint(\"Finished Training\")","metadata":{"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#nn.BCELoss()\n#outputs_squeeze","metadata":{"execution":{"iopub.status.busy":"2023-04-29T05:51:43.260972Z","iopub.status.idle":"2023-04-29T05:51:43.262626Z","shell.execute_reply.started":"2023-04-29T05:51:43.262336Z","shell.execute_reply":"2023-04-29T05:51:43.262366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Getting test loss on the trained model\n\ntest_loss = 0.0\ncorrect = 0\ntotal = 0\n\n#model.load_state_dict(torch.load(\"/kaggle/working/weights.pt\"))\nmodel.load_state_dict(torch.load(\"/kaggle/input/dl-fp-best-model-4-29-23/my_model.pt\"))\n\n\nwith torch.no_grad():\n    for data in test_dataloader:\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        \n        loss = criterion(outputs.squeeze(), labels.float())\n        test_loss += loss.item()\n\n        # Calculate accuracy\n        predicted = torch.round(outputs.squeeze())\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n# Log test loss and accuracy to Neptune\ntest_loss = test_loss / len(test_dataloader)\ntest_accuracy = correct / total\n#run[\"test/loss\"].log(test_loss)\n#run[\"test/accuracy\"].log(test_accuracy)\n\nprint(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n\n# Close the Neptune run after logging all metrics\n#run.stop()","metadata":{"execution":{"iopub.status.busy":"2023-04-30T06:27:30.850116Z","iopub.execute_input":"2023-04-30T06:27:30.850739Z","iopub.status.idle":"2023-04-30T06:28:40.710800Z","shell.execute_reply.started":"2023-04-30T06:27:30.850685Z","shell.execute_reply":"2023-04-30T06:28:40.709419Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Test Loss: 0.23175373461184112, Test Accuracy: 0.908768634104957\n","output_type":"stream"}]}]}